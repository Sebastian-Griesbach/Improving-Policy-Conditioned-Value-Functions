[logger]
~MODULE = tc_logging.torch_logger
~CLASS = SimpleLogger
log_dir = *log_dir

[log_reader]
~MODULE = tc_logging.torch_logger
~CLASS = SimpleLogReader
log_path = *logger.log_path

[log_handler]
~MODULE = tc_logging.log_handler
~CLASS = LogHandler
logger = *logger
log_reader = *log_reader

;4417 Parameters
[actor]
~MODULE = tc_modules.policy_nets
~CLASS = PendulumPolicyNet

[environment_maker]
~MODULE = tc_environments.environment_maker
~CLASS = EnvironmentMaker
name = "Pendulum-v0"

[environment]
~MODULE = gym.wrappers
~CLASS = TimeLimit
env = *environment_maker.environment
max_episode_steps = 200

[embedding_net]
~MODULE = tc_modules.mlp_embedding
~CLASS = FingerprintEmbedding
num_in_states = 256
policy_network_example = *actor
observation_space = *environment.observation_space
action_space = *environment.action_space
normalized_input = True

[critic]
~MODULE = tc_modules.policy_conditioned_value_functions
~CLASS = ComparingStartStatePolicyConditionedValueFunction
embedding_net = *embedding_net
evaluation_net_hidden_dims = [512, 512, 512]

[exploration_module]
~MODULE = tc_reinforcement_learning.exploration_module
~CLASS = EpsilonNormalExplorationPolicyModule
example_policy = *actor
noise_std = 0.2
epsilon = 0.1
random_prob = 0.1

[param_handler]
~MODULE = tc_utils.mlp_parameter_handler
~CLASS = NamedParamHandler
example_policy = *actor

[pcac]
~MODULE = tc_algorithms.policy_conditioned_actor_critic
~CLASS = ComparingStartStatePCAC
environment_state_shape = *environment.observation_space.shape
actor_param_handler = *param_handler
critic_optimizer_parameters = {"lr": 0.001}
actor_optimizer_parameters = {"lr": 0.0001}

[replaybuffer]
~MODULE = tc_reinforcement_learning.replay_buffer
~CLASS = NormalizingReplayBuffer
max_size = 1000
replay_buffer_info = *pcac.replay_buffer_info
to_normalize = ["returns"]

[environment_handler]
~MODULE = tc_reinforcement_learning.environment_handler
~CLASS = StartStateMonteCarloEnvironmentHandler
exploration_environment = *environment

[rollout_handler]
~MODULE = tc_reinforcement_learning.rollout_handler
~CLASS = ModuleRolloutHandler
environment_handler = *environment_handler
to_device = "cuda"

[algo_handler]
~MODULE = tc_reinforcement_learning.algorithm_handler
~CLASS = DefaultACAlgorithmHandler
algorithm = *pcac
actor = *actor
critic = *critic
device = "cuda"
exploration_module = *exploration_module
rollout_handler = *rollout_handler
replay_buffer = *replaybuffer
num_evaluation_runs = 50
batch_size = 32

[trainer]
~MODULE = tc_reinforcement_learning.trainer
~CLASS = ActorCriticTrainer
ac_algo = *algo_handler
log_handler = *log_handler
critic_update_freq = 1
critic_update_steps = 10
actor_update_freq = 1
actor_update_steps = 10
evaluation_freq = 100
warmup_steps = 10

[~RETURN]
RETURN = [trainer, environment, log_handler]