[logger]
~MODULE = tc_logging.torch_logger
~CLASS = SimpleLogger
log_dir = *log_dir

[log_reader]
~MODULE = tc_logging.torch_logger
~CLASS = SimpleLogReader
log_path = *logger.log_path

[log_handler]
~MODULE = tc_logging.log_handler
~CLASS = LogHandler
logger = *logger
log_reader = *log_reader

[actor]
~MODULE = tc_modules.policy_nets
~CLASS = PendulumPolicyNet

[environment_maker]
~MODULE = tc_environments.environment_maker
~CLASS = EnvironmentMaker
name = "Pendulum-v0"

[environment]
~MODULE = gym.wrappers
~CLASS = TimeLimit
env = *environment_maker.environment
max_episode_steps = 200

[embedding_net]
~MODULE = tc_modules.mlp_embedding
~CLASS = FlatMPLEmbedding
policy_network_example = *actor

[critic]
~MODULE = tc_modules.policy_conditioned_value_functions
~CLASS = StateActionPolicyConditionedValueFunction
embedding_net = *embedding_net
evaluation_net_hidden_dims = [512, 512, 512]
state_size = *environment.observation_space.shape
action_size = *environment.action_space.shape

[exploration_module]
~MODULE = tc_reinforcement_learning.exploration_module
~CLASS = EpsilonNormalExplorationPolicyModule
example_policy = *actor
noise_std = 0.4
epsilon = 0
random_prob = 0

[param_handler]
~MODULE = tc_utils.mlp_parameter_handler
~CLASS = FlatParamHandler
example_policy = *actor

[pcac]
~MODULE = tc_algorithms.policy_conditioned_actor_critic
~CLASS = PAVF
environment_state_shape = *environment.observation_space.shape
environment_action_shape = *environment.action_space.shape
actor_param_handler = *param_handler
critic_optimizer_parameters = {"lr": 0.0001}
actor_optimizer_parameters = {"lr": 0.0001}
discount = 0.99

[replaybuffer]
~MODULE = tc_reinforcement_learning.replay_buffer
~CLASS = RedundancyReplayBuffer
max_size = 2000000
replay_buffer_info = *pcac.replay_buffer_info
to_normalize = ["rewards", "observations"]
redundancy_keys = *pcac.policy_replay_keys
num_redundancys = 200

[environment_handler]
~MODULE = tc_reinforcement_learning.environment_handler
~CLASS = StateActionTDEnvironmentHandler
exploration_environment = *environment
evaluation_environment = *environment_maker.environment

[rollout_handler]
~MODULE = tc_reinforcement_learning.rollout_handler
~CLASS = ModuleRolloutHandler
environment_handler = *environment_handler
to_device = *device

[algo_handler]
~MODULE = tc_reinforcement_learning.algorithm_handler
~CLASS = DefaultACAlgorithmHandler
algorithm = *pcac
actor = *actor
critic = *critic
device = *device
exploration_module = *exploration_module
rollout_handler = *rollout_handler
replay_buffer = *replaybuffer
num_evaluation_runs = 50
batch_size = 32

[trainer]
~MODULE = tc_reinforcement_learning.trainer
~CLASS = ActorCriticTrainer
ac_algo = *algo_handler
log_handler = *log_handler
critic_update_freq = 1
critic_update_steps = 10
actor_update_freq = 1
actor_update_steps = 10
evaluation_freq = 100
warmup_steps = 10

[~RETURN]
RETURN = [trainer, environment, log_handler]