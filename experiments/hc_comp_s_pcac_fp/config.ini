[logger]
~MODULE = tc_logging.torch_logger
~CLASS = SimpleLogger
log_dir = *log_dir

[log_reader]
~MODULE = tc_logging.torch_logger
~CLASS = SimpleLogReader
log_path = *logger.log_path

[log_handler]
~MODULE = tc_logging.log_handler
~CLASS = LogHandler
logger = *logger
log_reader = *log_reader

[actor]
~MODULE = tc_modules.policy_nets
~CLASS = HalfCheetahNetLarge

[environment_maker]
~MODULE = tc_environments.environment_maker
~CLASS = EnvironmentMaker
name = "HalfCheetah-v2"

[environment]
~MODULE = gym.wrappers
~CLASS = TimeLimit
env = *environment_maker.environment
max_episode_steps = 300

[embedding_net]
~MODULE = tc_modules.mlp_embedding
~CLASS = FingerprintEmbedding
num_in_states = 220
policy_network_example = *actor
observation_space = *environment.observation_space
action_space = *environment.action_space

[critic]
~MODULE = tc_modules.policy_conditioned_value_functions
~CLASS = ComparingStatePolicyConditionedValueFunction
embedding_net = *embedding_net
evaluation_net_hidden_dims = [1024, 1024, 1024]
state_size = *environment.observation_space.shape

[exploration_module]
~MODULE = tc_reinforcement_learning.exploration_module
~CLASS = EpsilonNormalExplorationPolicyModule
example_policy = *actor
noise_std = 0.077
epsilon = 0.1
random_prob = 0

[param_handler]
~MODULE = tc_utils.mlp_parameter_handler
~CLASS = NamedParamHandler
example_policy = *actor

[pcac]
~MODULE = tc_algorithms.policy_conditioned_actor_critic
~CLASS = ComparingStatePCAC
environment_state_shape = *environment.observation_space.shape
actor_param_handler = *param_handler
critic_optimizer_parameters = {"lr": 0.001}
actor_optimizer_parameters = {"lr": 0.00005}

[replaybuffer]
~MODULE = tc_reinforcement_learning.replay_buffer
~CLASS = RedundancyReplayBuffer
max_size = 300000
replay_buffer_info = *pcac.replay_buffer_info
to_normalize = ["returns", "observations"]
redundancy_keys = *pcac.policy_replay_keys
num_redundancys = 300

[environment_handler]
~MODULE = tc_reinforcement_learning.environment_handler
~CLASS = StateMonteCarloEnvironmentHandler
exploration_environment = *environment
evaluation_environment = *environment_maker.environment
discount = 0.99

[rollout_handler]
~MODULE = tc_reinforcement_learning.rollout_handler
~CLASS = ModuleRolloutHandler
environment_handler = *environment_handler
to_device = *device

[algo_handler]
~MODULE = tc_reinforcement_learning.algorithm_handler
~CLASS = DefaultACAlgorithmHandler
algorithm = *pcac
actor = *actor
critic = *critic
device = *device
exploration_module = *exploration_module
rollout_handler = *rollout_handler
replay_buffer = *replaybuffer
num_evaluation_runs = 50
batch_size = 32

[trainer]
~MODULE = tc_reinforcement_learning.trainer
~CLASS = ActorCriticTrainer
ac_algo = *algo_handler
log_handler = *log_handler
critic_update_freq = 1
critic_update_steps = 16
actor_update_freq = 1
actor_update_steps = 1
evaluation_freq = 100
warmup_steps = 10

[~RETURN]
RETURN = [trainer, environment, log_handler]